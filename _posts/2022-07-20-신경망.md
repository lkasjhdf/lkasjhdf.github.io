## 뉴럴 네트워크 

- 인공신경망은 생물학적 뉴런들의 네트워크를 추상화된 기능적 레벨에서 모사한 것



## 선형 신경망

![1](C:\Users\서장원\Desktop\1.jpg)

- Data: $D={x_i,y_i}_{i=1}^N$

- Model: $\widehat{y}=wx+b$ #$x_i$를 집어넣었을 때 현재 모델에서 나오는 출력값

  - $w$와 $b$를 찾아야 함

- Loss: $loss=\frac{1}{N} \sum_{i=1}^N (y_i-\widehat{y}_i)^2$

  

$$
\frac{\partial loss}{\partial w}=\frac{\partial}{\partial w}\frac{1}{N}\sum_{i=1}^N (y_i-\widehat{y}_i)^2\\
=\frac{\partial}{\partial w}\frac{1}{N}\sum_{i=1}^N (y_i-wx_i-b)^2\\
=-\frac{1}{N}\sum_{i=1}^N -2(y_i-wx_i-b)x_i
$$

$$
\frac{\partial loss}{\partial b}=\frac{\partial}{\partial b}\frac{1}{N}\sum_{i=1}^N (y_i-\widehat{y}_i)^2\\
=\frac{\partial}{\partial b}\frac{1}{N}\sum_{i=1}^N (y_i-wx_i-b)^2\\
=-\frac{1}{N}\sum_{i=1}^N -2(y_i-wx_i-b)
$$

$$
w\leftarrow w-\eta\frac{\partial loss}{\partial w}\\
b\leftarrow b-\eta\frac{\partial loss}{\partial b}
$$

$\eta$가 너무 크거나 작으면 학습이 안됨






$$
\begin{bmatrix}
\\
\mathbf{y}\\
\\
\end{bmatrix}
=
\begin{bmatrix}
\\
\qquad W^{\top}\qquad \\
\\
\end{bmatrix}
\begin{bmatrix}
\\
\mathbf{x}\\
\\
\end{bmatrix}
+\begin{bmatrix}
\\
\mathbf{b}\\
\\
\end{bmatrix}
\\
\Longrightarrow
\begin{bmatrix}
\\
\mathbf{x}\\
\\
\end{bmatrix}=\left\{ W,\mathbf{b} \right\}\Rightarrow \begin{bmatrix}
\\
\mathbf{y}\\
\\
\end{bmatrix}
$$

- n차원에서 m차원으로 가는 모델은 행렬을 사용하여 표현 가능(서로 다른 차원 사이의 선형 변환을 찾겠다)





## 층을 여러개 쌓으려면

$$
\begin{bmatrix}
\\
\mathbf{x}\\
\\
\end{bmatrix}=W_1\Rightarrow \begin{bmatrix}
\\
\mathbf{h}\\
\\
\end{bmatrix}
=W_2\Rightarrow \begin{bmatrix}
\\
\mathbf{y}\\
\\
\end{bmatrix}
$$

$$
y=W^{\top}_2\mathbf{h}=W^{\top}_2W^{\top}_1\mathbf{x}
$$

하지만 이러면 선형 하나로 표현된 한 단짜리 네트워크와 다를 게 없다




$$
y=W^{\top}_2\mathbf{h}=W^{\top}_2\rho(W^{\top}_1\mathbf{x})
$$
$\rho$라는 비선형의 **활성화 함수** 곱해주기



## 활성화 함수들

![img](C:\Users\서장원\Desktop\img.png)

- 문제마다 상황마다 좋은 게 다름

  

