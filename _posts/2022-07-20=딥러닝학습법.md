---
layout: post
title: "[4] 딥러닝학습법"
use_math: true
---

각 행백터 $\mathbf{o}_i$는 데이터 $\mathbf{x}__i$와 가중치 행렬 $\mathbf{W}$ 사이의 행렬곱과 절편 $\mathbf{b}$ 벡터의 합으로 표현된다고 가정해보자



$\mathbf{O}$는 $\mathbf{(n*p)}$

$\mathbf{X}$는 $\mathbf{(n*d)}$

$\mathbf{W}$는 $\mathbf{(d*p)}$

$\mathbf{b}$는 $\mathbf{(n*p)}$
$$
\mathbf{O}=\mathbf{X}\mathbf{W}+\mathbf{b}\\\\

\begin{bmatrix}
\mathbf{o}_{11}\\
\mathbf{o}_{21}\\
\vdots\\
\mathbf{o}_{n1}\\
\end{bmatrix}
=\begin{bmatrix}
\mathbf{x}_{11}\\
\mathbf{x}_{21}\\
\vdots\\
\mathbf{x}_{n1}\\
\end{bmatrix}
\begin{bmatrix}
w_{11}\quad w_{12}\quad \cdots \quad w_{1p} \\
w_{21}\quad w_{22}\quad \cdots \quad w_{2p} \\
\vdots \qquad \vdots \qquad\qquad\quad \vdots \\\ 
w_{d1}\quad w_{d2}\quad \cdots \quad w_{dp} \\
\end{bmatrix}
+\begin{bmatrix}
\mid \quad\ \mid \quad \cdots \quad \mid \ \\
b_1 \quad b_2 \quad \cdots \quad b_p\\
\mid \quad\ \mid \quad \cdots \quad \mid \ \\
\end{bmatrix}
$$
데이터가 바뀌면 결과값도 바뀌게 됨. 이 때 출력 벡터의 차원은 $d$에서 $p$로 바뀌게 됨

![1](C:\Users\서장원\Desktop\1.jpg)



출력벡터 $\mathbf{o}$에 softmax 함수를 합성하면 확률벡터가 되므로 특정 클래스 k에 속할 확률로도 해석이 가능함
$$
softmax(\mathbf{o})=( \frac{\exp(o_1)}{\sum_{k=1}^p\exp(o_k)},\cdots,\frac{\exp(o_p)}{\sum_{k=1}^p\exp(o_k)} )
$$


## 소프트맥스 연산

- 모델의 출력을 확률로 해석할 수 있도록 변환해 주는 연산
- 분류 문제를 풀 때 선형모델과 소프트맥스 함수를 결합하여 예측함
- 추론을 할 땐 원-핫 벡터로 최대값을 가진 주소만 1로 출력하는 연산을 사용하여 소프트연산을 사용하지는 않음 -> one_hot(softmax(o))   (X)     one_hot(o)   (O)

## 신경망을 수식으로 분해해보자

- 신경망은 선형모델과 활성화 함수를 합성한 함수
- 활성화 함수는 비선형함수로 활성화 함수를 쓰지 않으면 딥러닝은 선형모델과 차이가 없음
- 종류: sigmoid, tanh, ReLU(딥러닝에서 제일 많이 씀)

![KakaoTalk_20220720_020024654](C:\Users\서장원\Desktop\KakaoTalk_20220720_020024654.jpg)

- 잠재백터 $\mathbf{z}=(z_1,\cdots ,z_q)$의 각 노드에 개별적으로 적용하여 새로운 잠재벡터 $\mathbf{H}=(\sigma(\mathbf{z}_1), \cdots, \sigma(\mathbf{z}_n))$를 만든다



![KakaoTalk_20220720_021156583](C:\Users\서장원\Desktop\KakaoTalk_20220720_021156583.jpg)
$$
\mathbf{O}=\mathbf{H}\mathbf{W}^{(2)}+\mathbf{b}^{(2)}\\
\mathbf{H}=(\sigma(\mathbf{z}_1),\cdots,\sigma(\mathbf{z}_n))\\
\sigma(\mathbf{z})=\sigma(\mathbf{W}^{(1)}\mathbf{x}+\mathbf{b}^{(1)})
$$


![KakaoTalk_20220720_020824354](C:\Users\서장원\Desktop\KakaoTalk_20220720_020824354.jpg)
$$
\mathbf{O}=\mathbf{Z}^{(L)}\\
\vdots\\
\mathbf{H}^{(l)}=\sigma(\mathbf{Z}^{(l)})\\
\mathbf{Z}^{(l)}=\mathbf{H}^{(l-1)}\mathbf{W}^{(l)}+\mathbf{b}^{(l)}\\
\vdots\\
\mathbf{H}^{(1)}=\sigma(\mathbf{Z}^{(1)})\\
\mathbf{Z}^{(1)}=\mathbf{X}\mathbf{W}^{(1)}+\mathbf{b}^{(1)}\\
$$

- $l=1,\vdots,L$까지 순차적인 신경망 계산을 ""순전파""라 부름



## 층을 여러개 쌓는 이유

- 층이 깊을수록 목적함수를 근사하는데 필요한 뉴런의 숫자가 훨씬 빨리 줄어듦(최적화가 더 쉬운건 아님)

## 역전파 알고리즘

- 딥러닝에선 층별로 쌓아서 계산을 하기에 한번에 그라디언트 계산이 불가함. 따라서 역전파 알고리즘을 통해 각 층에 사용된 파라메터${\mathbf{W}^{(l)}, \mathbf{b}^{(l)}}_{l=1}^{L}$를 학습함
- 각 층의 파라메터의 그레디언트 벡터는 윗층부터 역순으로 계산하게 됨
- 연쇄법칙 사용
- 각 뉴런에 해당하는 값(텐서)가 컴퓨터에 메모리로 저장되어 있어야 역전파 알고리즘이 동작함$\rightarrow$ 순전파보다 메모리 많이 사용
