---
layout: post
title: "[3] 경사하강법"
use_math: true
---
# 경사 하강법

## 미분이란?

- 미분(differentiation)은 변수의 움직임에 따른 함수값의 변화를 측정하기 위한 도구로 최적화에서 가장 많이 쓰는 기법
  $$
  f^\prime(x)=\lim_{n \to 0}	\frac{f(x+h)-f(x)}{h}
  $$

![1](https://user-images.githubusercontent.com/90087083/179637494-38cee096-571c-4219-a2a4-2b2cd9ec9bd8.jpg){: width="50%" height="50%"}


- 미분은 함수 f의 주어진 점 (x,f(x))에서의 접선의 기울기를 구함
- 한 점에서 접선의 기울기를 알면 어느 방향으로 점을 움직여야 함수값이 증가/ 감소하는지 알 수 있다
- 미분값을 더하면 경사상승법이라 하며 극대값의 위치를 구할 때 사용
- 미분값을 빼면 경사하강법이라 하며 극소값의 위치를 구할 때 사용
- 경사상승/하강법은 극값에 도달하면 움직임을 멈춤



## 경사하강법 알고리즘

```
Input: gradient, init, lr, eps, Output: var
____________________________________________
#gradient: 미분을 시작하는 함수
#init: 시작점, lr: 학습률, eps: 알고리즘 종료조건

var = init
grad = gradient(var)
while(abs(grad) > eps):
	var = var - lr * grad
	grad = gradient(var)
```



```python
#함수가 f(x)=x^2+2x+3일 때 python 코드
def func(val):
    fun=sym.poly(x**2 + 2*x + 3)
    return fun.subs(x, val), fun

def func_gradient(fun, val):
    _, function = fun(val)
    diff = sym.diff(function, x)
    return diff.subs(x, val), diff

def gradient_descent(fun, init_point, lr_rate=1e-2, epsilon=1e-5):
    cnt = 0
    val = init_point
    diff, _ = func_gradient(fun, init_point)
    while np.abs(diff) > epsilon:
        val = val - lr_rate*diff
        diff, _ = func_gradient(fun, val)
        cnt+=1
        
    print("함수: {}, 연산횟수: {}, 최소점: ({}, {})".format(fun(val)[1], cnt, val, fun(val)[0]))
gradient_descent(fun=func, init_point=np.random.uniform(-2,2))
```



## 변수가 벡터이면

- 벡터가 입력인 다변수 함수의 경우 편미분을 사용
  $$
  \partial_{xi} f^\prime(\mathbf{x})=\lim_{n \to 0}	\frac{f(\mathbf{x}+h\mathbf{e}_i)-f(\mathbf{x})}{h}
  \\{\mathbf{e}_i \scriptstyle\text{ 는 i번째 값만 1이고 나머지는 0인 단위벡터}}
  $$

- 각 변수 별로 편미분을 계산한 그레디언트 벡터를 이용하여 경사하강/ 경사상승법에 사용할 수 있다
  $$
  \nabla f= (\partial_{x1}f,\partial_{x2}f,\cdots,\partial_{xd}f )
  \\{\nabla \scriptstyle\text{를 nabla 라고 부름}}
  $$



## 경사하강법으로 선형회귀 계수 구하기

$$
선형회귀의\ 목적식은\ \begin{Vmatrix}\mathbf{y}- \mathbf{X}\beta \end{Vmatrix}_2이고 \ 이를 최소화하는\ \beta를\ 찾아야\ 하므로\\ 다음과\ 같은\ 그레디언트\ 벡터를\ 구해야\ 한다
$$




$$
\nabla_{\beta}\begin{Vmatrix}\mathbf{y}- \mathbf{X}\beta \end{Vmatrix}_2
= (\partial_{\beta_1}\begin{Vmatrix}\mathbf{y}- \mathbf{X}\beta \end{Vmatrix}_2,\cdots,\partial_{\beta_d}\begin{Vmatrix}\mathbf{y}- \mathbf{X}\beta \end{Vmatrix}_2)
\\ { \scriptstyle\text {$\begin{Vmatrix}\mathbf{y}- \mathbf{X}\beta \end{Vmatrix}_2$가 아닌 $\begin{Vmatrix}\mathbf{y}- \mathbf{X}\beta \end{Vmatrix}^2_2$를 최소화 해도 된다}}
$$




$$
\partial_{\beta_k}\begin{Vmatrix}\mathbf{y}- \mathbf{X}\beta \end{Vmatrix}_2=\partial_{\beta_k}\left\{ \frac{1}{n}\sum_{i=1}^n	\left( y_i-\sum_{j=1}^dX_{ij}\beta_j \right)^2 \right\}^{1/2}
\\=-\frac{\mathbf{X}^\top_{\cdot k}(\mathbf{y}-\mathbf{X}\beta)}{n\begin{Vmatrix}\mathbf{y}- \mathbf{X}\beta \end{Vmatrix}_2}
$$


$$
{ \scriptstyle\text {}}
$$


$$
\nabla_{\beta}\begin{Vmatrix}\mathbf{y}- \mathbf{X}\beta \end{Vmatrix}_2
= (\partial_{\beta_1}\begin{Vmatrix}\mathbf{y}- \mathbf{X}\beta \end{Vmatrix}_2,\cdots,\partial_{\beta_d}\begin{Vmatrix}\mathbf{y}- \mathbf{X}\beta \end{Vmatrix}_2)
\\ =\left( -\frac{\mathbf{X}^\top_{\cdot 1}(\mathbf{y}-\mathbf{X}\beta)}{n\begin{Vmatrix}\mathbf{y}- \mathbf{X}\beta \end{Vmatrix}_2},\cdots,-\frac{\mathbf{X}^\top_{\cdot d}(\mathbf{y}-\mathbf{X}\beta)}{n\begin{Vmatrix}\mathbf{y}- \mathbf{X}\beta \end{Vmatrix}_2} \right)=\left( -\frac{\mathbf{X}^\top(\mathbf{y}-\mathbf{X}\beta)}{n\begin{Vmatrix}\mathbf{y}- \mathbf{X}\beta \end{Vmatrix}_2}\right)
$$






$$
이제\ 목적식을\ 최소화하는\ \beta를\ 구하는\ 경사하강법\ 알고리즘은\ 다음과\ 같다
\\ \beta^{(t+1)}\leftarrow \beta^{(t)}-\lambda \nabla_{\beta}\begin{Vmatrix}\mathbf{y}- \mathbf{X}\beta^{(t)} \end{Vmatrix}
\quad {\scriptstyle\text{$\nabla$는 수렴속도 결정}}
\\ \beta^{(t+1)}\leftarrow \beta^{(t)}+\frac{\lambda}{n}\frac{\mathbf{X}^{\top}(\mathbf{y}-\mathbf{X}\beta^{(t)})}
{% raw %}{{\begin{Vmatrix} \mathbf{y}- \mathbf{X}\beta^{(t)} \end{Vmatrix}}}{% endraw %}\quad {\scriptstyle\text{$\lambda$앞에 붙은 -부호가 +부호로 바뀐것에 주의}}
$$








$$
\begin{Vmatrix}\mathbf{y}- \mathbf{X}\beta \end{Vmatrix}_2대신\ \begin{Vmatrix}\mathbf{y}- \mathbf{X}\beta \end{Vmatrix}_2^2를\ 최소화\ 하면\ 간단해진다
$$


$$
\nabla_{\beta}\begin{Vmatrix}\mathbf{y}- \mathbf{X}\beta \end{Vmatrix}_2^2
= (\partial_{\beta_1}\begin{Vmatrix}\mathbf{y}- \mathbf{X}\beta \end{Vmatrix}_2^2,\cdots,\partial_{\beta_d}\begin{Vmatrix}\mathbf{y}- \mathbf{X}\beta \end{Vmatrix}_2^2)
\\ =-\frac{2}{n}\mathbf{X}^{\top}(\mathbf{y}- \mathbf{X}\beta )
$$


$$
이제\ 목적식을\ 최소화하는\ \beta를\ 구하는\ 경사하강법\ 알고리즘은\ 다음과\ 같다
\\ \beta^{(t+1)}\leftarrow \beta^{(t)}+\frac{2\lambda}{n}\mathbf{X}^{\top}(\mathbf{y}- \mathbf{X}\beta^{(t)})
$$



## 경사하강법 기반 선형회귀 알고리즘

```
Input: X, y, lr, T, Output: beta
____________________________________________
#norm: L2-norm을 계산하는 함수
#lr: 학습률, T: 학습횟수

for t i range(T):
	error: y-xX @ beta
    grad = - transpose(X) @ error
    beta = beta - lr *grad
```



## 경사하강법은 만능일까?

- 이론적으로 경사하강법은 미분가능하고 볼록(convex)한 함수에 대해선 적절한 학습률과 학습횟수를 선택했을 때 수렴이 보장되어있음
- 선형회귀의 경우도 회귀계수에 대해 볼록함수라 수렴이 보장됨
- 비선형회귀 문제의 경우 목적식이 볼록하지 않을 수 있으므로 수렴이 항상 보장되지는 않음



## 확률적 경사하강법(SGD)

- 확률적 경사하강법(Stochastic Gradient Descent)은 모든 데이터를 사용해서 업데이트 하는 대신 데이터 한개 또는 일부를 이용하여 업데이트함
- 볼록이 아닌(non-convex) 목적식은 SGD를 통해 최적화 가능함



## 미니배치 경사 하강법

- SGD는 미니배치를 가지고 그레디언트 벡터 계산
- 미니배치는 확률적으로 선택하므로 목적식의 모양이 바뀜
