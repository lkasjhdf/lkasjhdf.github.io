# 최적화 기본 용어

## 최적화에 있어서 중요한 개념들

- Generalization 
- Under-fitting vs. Over-fitting 
- Cross validation 
- Bias-variance tradeoff 
- Bootstrapping 
- Bagging and Boosting



## Generalization 

![1](C:\Users\서장원\Desktop\최적화 1\1.JPG)

Generalization performance가 좋다 -> 네트워크 성능이 학습 데이터와 비슷하게 나온다





## Under-fitting vs. Over-fitting 

- Over-fitting: 학습 데이터만 다 맞춤
- Under-fitting : 학습 데이터마저 못맞춤



## Cross validation 

![2](C:\Users\서장원\Desktop\최적화 1\2.jpg)

Cross validation: 학습 데이터를 k개로 나눈 후 (k-1)개로 학습을 시키고 1개로 검증을 해보는 것



## Bias and Variance

- variance: 출력이 얼마나 일관적으로 나오는지
  - variance 크다: 비슷한 입력이라도 출력이 많이 달라짐
- bias: 평균적으로 얼마나 목표에 접근하는지
  - bias 높다: 원하고자 하는 값과 많이 벗어나 있다



## Bias-variance tradeoff 

학습 데이터에 Noise가 껴있을 경우

Bias를 줄이게 되면 Variance가 높아질 가능성이 크고

Variance를 줄이는 모델은 Bias가 높아질 가능성이 크다



## Bootstrapping 

학습 데이터가 고정되어 있을 때 subsampling을 통해 학습 데이터를 여러 개를 만들고 그것으로 여러 모델을 만들겠다



## Bagging and Boosting

- Bagging (Bootstrapping aggregating)
  -  여러 개의 모델을 subsampling을 통해 만들고 여러 모델들로 결과가 나오면 그 결과들을 평균을 내겠다
-  Boosting
  - 간단한 모델을 만들고 이 모델이 잘 작동하지 않는 데이터에 대한 모델을 만드는 것을 반복하여 여러 개의 모델을 만들어서 이 모델들을 sequential 하게 합쳐서 하나의 모델을 만듦